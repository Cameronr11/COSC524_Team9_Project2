{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJWe40Kqpe9-",
        "outputId": "c22bf7cd-2b48-419c-d0df-fdfe283d9db5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec (from torch)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch datasets\n",
        "from transformers import BertTokenizer, BertModel, pipeline, AutoTokenizer, AutoModelForQuestionAnswering, Trainer, TrainingArguments, AutoModelForQuestionAnswering\n",
        "from datasets import load_dataset, Dataset\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import re\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_text(file_path):\n",
        "    \"\"\"\n",
        "    Load text from a file.\n",
        "    \"\"\"\n",
        "    print(\"Loading the text from the file...\")\n",
        "    with open(book_path, 'r', encoding='utf-8') as file:\n",
        "        return file.read()\n",
        "\n",
        "def remove_gutenberg_header_footer(text, start_marker, end_marker):\n",
        "    \"\"\"\n",
        "    Remove the header and footer from Project Gutenberg text.\n",
        "\n",
        "    Finds specified start and end markers and returns the content in between.\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The text to clean.\n",
        "        start_marker (str): Marker indicating the start of main content.\n",
        "        end_marker (str): Marker indicating the end of main content.\n",
        "\n",
        "    Returns:\n",
        "        str: Text with header and footer removed, or original text if markers are not found.\n",
        "    \"\"\"\n",
        "    start_index = text.find(start_marker)\n",
        "    if start_index == -1:\n",
        "        return text\n",
        "\n",
        "    end_index = text.find(end_marker)\n",
        "    if end_index == -1:\n",
        "        return text\n",
        "\n",
        "    cleaned_text = text[start_index + len(start_marker):end_index].strip()\n",
        "    return cleaned_text\n",
        "\n",
        "book_path = \"/content/drive/My Drive/Cosc524 - Collaboration/a study in scarlet.txt\"\n",
        "\n",
        "# here we are cleaning the text and removing the headers and footers\n",
        "raw_text = load_text(book_path)\n",
        "\n",
        "start_marker = \"*** START OF THE PROJECT GUTENBERG EBOOK A STUDY IN SCARLET ***\"\n",
        "end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK A STUDY IN SCARLET ***\"\n",
        "\n",
        "cleaned_text = remove_gutenberg_header_footer(raw_text, start_marker, end_marker)\n",
        "\n",
        "print(\"Cleaned text sample:\\n\")\n",
        "print(cleaned_text[:500])\n",
        "\n",
        "def tokenize_sentences(text):\n",
        "    \"\"\"\n",
        "    Tokenize the input text into sentences using NLTK.\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The text to tokenize.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of tokenized sentences.\n",
        "    \"\"\"\n",
        "    print(\"Tokenizing the text into sentences...\")\n",
        "    sentences = sent_tokenize(text)\n",
        "    print(f\"Total sentences: {len(sentences)}\")\n",
        "    return sentences\n",
        "\n",
        "my_sentences = tokenize_sentences(cleaned_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aplu0ANdyoww",
        "outputId": "f41010ef-bebd-4996-8286-e65a6387ed79"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the text from the file...\n",
            "Cleaned text sample:\n",
            "\n",
            "A STUDY IN SCARLET\n",
            "\n",
            "By A. Conan Doyle\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "CONTENTS\n",
            "\n",
            " A STUDY IN SCARLET.\n",
            "\n",
            " PART I.\n",
            " CHAPTER I. MR. SHERLOCK HOLMES.\n",
            " CHAPTER II. THE SCIENCE OF DEDUCTION.\n",
            " CHAPTER III. THE LAURISTON GARDENS MYSTERY\n",
            " CHAPTER IV. WHAT JOHN RANCE HAD TO TELL.\n",
            " CHAPTER V. OUR ADVERTISEMENT BRINGS A VISITOR.\n",
            " CHAPTER VI. TOBIAS GREGSON SHOWS WHAT HE CAN DO.\n",
            " CHAPTER VII. LIGHT IN THE DARKNESS.\n",
            "\n",
            " PART II. THE COUNTRY OF THE SAINTS\n",
            " CHAPTER I. ON THE GREAT ALKALI PLAIN.\n",
            " CHAPTER II. THE FLOWER OF UTAH.\n",
            " CHAPTER III. J\n",
            "Tokenizing the text into sentences...\n",
            "Total sentences: 2208\n",
            "Number of tokenized sentences: 2208\n",
            "\n",
            "Sample tokenized sentences:\n",
            "1: A STUDY IN SCARLET\n",
            "\n",
            "By A. Conan Doyle\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "CONTENTS\n",
            "\n",
            " A STUDY IN SCARLET.\n",
            "2: PART I.\n",
            "3: CHAPTER I. MR. SHERLOCK HOLMES.\n",
            "4: CHAPTER II.\n",
            "5: THE SCIENCE OF DEDUCTION.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For text generation\n",
        "generator = pipeline('fill-mask', model='bert-base-uncased')\n",
        "\n",
        "# For text retrieval\n",
        "retriever_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "retriever_model = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhD7gkdhpi_m",
        "outputId": "2e3e0ee6-89c2-434d-e259-703eead81c9e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_relevant_info(query, top_k=5, embeddings=None, sentences=None):\n",
        "\n",
        "  \"\"\"Retrieves the most relevant sentences based on cosine similarity.\n",
        "\n",
        "  Args:\n",
        "    query: The input query string.\n",
        "    top_k: The number of top sentences to retrieve.\n",
        "\n",
        "  Returns:\n",
        "    A list of the top_k most relevant sentences.\n",
        "  \"\"\"\n",
        "  query_embedding = retriever_model(**retriever_tokenizer(query, return_tensors=\"pt\")).last_hidden_state[:, 0, :]\n",
        "  similarities = F.cosine_similarity(embeddings, query_embedding, dim=1) # Assume 'embeddings' is pre-calculated\n",
        "  top_indices = torch.topk(similarities, top_k).indices\n",
        "  relevant_sentences = [sentences[idx] for idx in top_indices]  # Assume 'sentences' is pre-calculated\n",
        "  return relevant_sentences"
      ],
      "metadata": {
        "id": "Wjmdx5N4rmI5"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_with_rag(prompt, top_k=5, embeddings=None, sentences=None):\n",
        "  \"\"\"Generates text using RAG with BERT.\n",
        "\n",
        "  Args:\n",
        "    prompt: The input prompt for text generation.\n",
        "    top_k: The number of retrieved sentences to use for augmentation.\n",
        "\n",
        "  Returns:\n",
        "    The generated text.\n",
        "  \"\"\"\n",
        "  relevant_info = retrieve_relevant_info(prompt, top_k=top_k, embeddings=embeddings, sentences=sentences)\n",
        "  augmented_prompt = f\"{prompt} [SEP] {' '.join(relevant_info)} [MASK].\"  # Add [MASK] token\n",
        "  generated_text = generator(augmented_prompt)[0]['sequence']\n",
        "  return generated_text"
      ],
      "metadata": {
        "id": "ly_4yQ3crsQM"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_embeddings = torch.load('/content/drive/My Drive/Cosc524 - Collaboration/embeddings/bert_embeddings.pt')\n",
        "prompt = \"Who was the murderer?\"\n",
        "generated_text = generate_text_with_rag(prompt=prompt, top_k=5, embeddings=my_embeddings, sentences=my_sentences)\n",
        "# print(generated_text)\n",
        "for sentence in sent_tokenize(generated_text):\n",
        "  print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vo1El1bruMu",
        "outputId": "ff0de441-2443-4dfe-8143-0c8e7785cc56"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-90129c60a95d>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  my_embeddings = torch.load('/content/drive/My Drive/Cosc524 - Collaboration/embeddings/bert_embeddings.pt')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "who was the murderer?\n",
            "where did the blood come from?\n",
            "why should he fear a trap?\n",
            "but why should he come back to the house after leaving it?\n",
            "what was that?\n",
            "how came the woman ’ s ring there?\n",
            "no.\n"
          ]
        }
      ]
    }
  ]
}