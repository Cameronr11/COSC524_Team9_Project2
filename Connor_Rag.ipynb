{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cf0755db99874f288d715fd44c7fc1ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_09fb4272bcbf42078e484f8ab4334215",
              "IPY_MODEL_270dcc2a8a654076ab627a460cc9394a",
              "IPY_MODEL_f0d8ea372f7b40a8916dd136801b703d"
            ],
            "layout": "IPY_MODEL_7f0e562924a44f43ae253e7e24caa5c9"
          }
        },
        "09fb4272bcbf42078e484f8ab4334215": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_917b43c350e34ba29f691e369dd0fe16",
            "placeholder": "​",
            "style": "IPY_MODEL_75c2381ca4c34ce5b3b8b57db380edf5",
            "value": "Batches: 100%"
          }
        },
        "270dcc2a8a654076ab627a460cc9394a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1bb17a51a95e4719bc27e2147e1bb82d",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ab94a5004b50496ca45242f69b937389",
            "value": 8
          }
        },
        "f0d8ea372f7b40a8916dd136801b703d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87d1f5ff585347df88d70759228f4ee8",
            "placeholder": "​",
            "style": "IPY_MODEL_48c384ab2d1546b6b98f7d3e775b6986",
            "value": " 8/8 [00:47&lt;00:00,  4.33s/it]"
          }
        },
        "7f0e562924a44f43ae253e7e24caa5c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "917b43c350e34ba29f691e369dd0fe16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75c2381ca4c34ce5b3b8b57db380edf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1bb17a51a95e4719bc27e2147e1bb82d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab94a5004b50496ca45242f69b937389": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "87d1f5ff585347df88d70759228f4ee8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48c384ab2d1546b6b98f7d3e775b6986": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJWe40Kqpe9-",
        "outputId": "58c1f7b9-ba6b-47bf-a667-b367ab693160"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch datasets\n",
        "!pip install tqdm\n",
        "from tqdm import tqdm\n",
        "from transformers import BertTokenizer, BertModel, pipeline, AutoTokenizer, AutoModelForQuestionAnswering, Trainer, TrainingArguments, AutoModelForQuestionAnswering, BertForQuestionAnswering\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from datasets import load_dataset, Dataset\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import re\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize text at sentence level for BERT tokenizer.  This is generic and will run for any relevant path so do it first.\n"
      ],
      "metadata": {
        "id": "7X_Hzkcmv2gb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_text(file_path):\n",
        "    print(\"Loading the text from the file...\")\n",
        "    with open(book_path, 'r', encoding='utf-8') as file:\n",
        "        return file.read()\n",
        "\n",
        "def remove_gutenberg_header_footer(text, start_marker, end_marker):\n",
        "    start_index = text.find(start_marker)\n",
        "    if start_index == -1:\n",
        "        return text\n",
        "\n",
        "    end_index = text.find(end_marker)\n",
        "    if end_index == -1:\n",
        "        return text\n",
        "\n",
        "    cleaned_text = text[start_index + len(start_marker):end_index].strip()\n",
        "    return cleaned_text\n",
        "\n",
        "book_path = \"/content/drive/My Drive/Cosc524 - Collaboration/a study in scarlet.txt\"\n",
        "\n",
        "# here we are cleaning the text and removing the headers and footers\n",
        "raw_text = load_text(book_path)\n",
        "\n",
        "print(raw_text[0:20])\n",
        "\n",
        "start_marker = \"*** START OF THE PROJECT GUTENBERG EBOOK A STUDY IN SCARLET ***\"\n",
        "end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK A STUDY IN SCARLET ***\"\n",
        "\n",
        "cleaned_text = remove_gutenberg_header_footer(raw_text, start_marker, end_marker)\n",
        "\n",
        "print(\"Cleaned text sample:\\n\")\n",
        "\n",
        "cleaned_text = cleaned_text[740:]\n",
        "print(cleaned_text[0:725])\n",
        "\n",
        "def tokenize_sentences(text):\n",
        "    print(\"Tokenizing the text into sentences...\")\n",
        "    sentences = sent_tokenize(text)\n",
        "    print(f\"Total sentences: {len(sentences)}\")\n",
        "    return sentences\n",
        "\n",
        "#Removing the table of contents so that RAG no longer tries to generate context with them.\n",
        "my_sentences = tokenize_sentences(cleaned_text)\n",
        "my_sentences = my_sentences[29:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aplu0ANdyoww",
        "outputId": "d0bd8841-6185-4c0e-c85d-f0c1f219c547"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the text from the file...\n",
            "﻿The Project Gutenbe\n",
            "Cleaned text sample:\n",
            "\n",
            "_Being a reprint from the Reminiscences of_ JOHN H. WATSON, M.D.,\n",
            "_Late of the Army Medical Department._)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "CHAPTER I.\n",
            "MR. SHERLOCK HOLMES.\n",
            "\n",
            "\n",
            "In the year 1878 I took my degree of Doctor of Medicine of the\n",
            "University of London, and proceeded to Netley to go through the course\n",
            "prescribed for surgeons in the army. Having completed my studies there,\n",
            "I was duly attached to the Fifth Northumberland Fusiliers as Assistant\n",
            "Surgeon. The regiment was stationed in India at the time, and before I\n",
            "could join it, the second Afghan war had broken out. On landing at\n",
            "Bombay, I learned that my corps had advanced through the passes, and\n",
            "was already deep in the enemy’s country. I followed, however, with many\n",
            "other officers who were i\n",
            "Tokenizing the text into sentences...\n",
            "Total sentences: 2179\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recursive Chunking Implementation"
      ],
      "metadata": {
        "id": "0zx-_q85JqJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Initialize the tokenizer for 'all-MiniLM-L6-v2'\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Set the maximum token limit for each chunk (you can adjust it as needed)\n",
        "MAX_TOKENS = 128\n",
        "\n",
        "def recursive_chunk_with_overlap(text, max_tokens=128, overlap=50):\n",
        "    \"\"\"\n",
        "    Chunk text with a specified overlap between chunks.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text to chunk.\n",
        "        max_tokens (int): Maximum number of tokens allowed per chunk.\n",
        "        overlap (int): Number of overlapping tokens between chunks.\n",
        "\n",
        "    Returns:\n",
        "        list[str]: A list of overlapping text chunks.\n",
        "    \"\"\"\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "\n",
        "    if len(tokens) <= max_tokens:\n",
        "        return [text]\n",
        "\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(tokens):\n",
        "        end = min(start + max_tokens, len(tokens))\n",
        "        chunk = tokenizer.convert_tokens_to_string(tokens[start:end])\n",
        "        chunks.append(chunk)\n",
        "        start += max_tokens - overlap  # Slide window with overlap\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "# Example usage: Chunk the entire book\n",
        "\n",
        "text_chunks = recursive_chunk_with_overlap(cleaned_text)\n",
        "\n",
        "# Print the number of chunks and a sample chunk\n",
        "print(f\"Number of chunks: {len(text_chunks)}\")\n",
        "print(f\"Sample chunk: {text_chunks[0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzqlBjfZJtZU",
        "outputId": "f4c4dd17-5b7d-49ce-e0d4-21cd83fe7f86"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (55708 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of chunks: 715\n",
            "Sample chunk: _ being a reprint from the reminiscences of _ john h. watson, m. d., _ late of the army medical department. _ ) chapter i. mr. sherlock holmes. in the year 1878 i took my degree of doctor of medicine of the university of london, and proceeded to netley to go through the course prescribed for surgeons in the army. having completed my studies there, i was duly attached to the fifth northumberland fusiliers as assistant surgeon. the regiment was stationed in india at the time, and before i could join it, the second afghan war had broken out. on landing at bombay, i learned that\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Document Based Chunking\n",
        "\n",
        "This chunks based on sentences or paragraphs"
      ],
      "metadata": {
        "id": "zkkZKFYsNtH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import re\n",
        "\n",
        "# Initialize the tokenizer for 'all-MiniLM-L6-v2' model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Set the maximum token limit for each chunk (adjust as needed)\n",
        "MAX_TOKENS = 256\n",
        "\n",
        "def chunk_document_by_paragraph(text, max_tokens=MAX_TOKENS):\n",
        "\n",
        "    # Split the document into paragraphs\n",
        "    paragraphs = text.split('\\n')  # Assuming paragraphs are separated by newlines\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "\n",
        "    # Iterate through the paragraphs and chunk them\n",
        "    for paragraph in paragraphs:\n",
        "        # Tokenize the paragraph\n",
        "        tokenized_paragraph = tokenizer.tokenize(paragraph)\n",
        "\n",
        "        # If adding this paragraph exceeds the token limit, start a new chunk\n",
        "        if len(tokenized_paragraph) + sum(len(tokenizer.tokenize(p)) for p in current_chunk) > max_tokens:\n",
        "            if current_chunk:  # If there's a current chunk, add it to the chunks list\n",
        "                chunks.append(' '.join(current_chunk))\n",
        "            current_chunk = [paragraph]  # Start a new chunk with the current paragraph\n",
        "        else:\n",
        "            current_chunk.append(paragraph)\n",
        "\n",
        "    # Add the final chunk if any paragraphs remain\n",
        "    if current_chunk:\n",
        "        chunks.append(' '.join(current_chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def chunk_document_by_sentence(text, max_tokens=MAX_TOKENS):\n",
        "\n",
        "    # Split the document into sentences\n",
        "    sentences = re.split(r'(?<=\\.)\\s+', text)  # Split by periods followed by a space (sentence delimiter)\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "\n",
        "    # Iterate through the sentences and chunk them\n",
        "    for sentence in sentences:\n",
        "        # Tokenize the sentence\n",
        "        tokenized_sentence = tokenizer.tokenize(sentence)\n",
        "\n",
        "        # If adding this sentence exceeds the token limit, start a new chunk\n",
        "        if len(tokenized_sentence) + sum(len(tokenizer.tokenize(s)) for s in current_chunk) > max_tokens:\n",
        "            if current_chunk:  # If there's a current chunk, add it to the chunks list\n",
        "                chunks.append(' '.join(current_chunk))\n",
        "            current_chunk = [sentence]  # Start a new chunk with the current sentence\n",
        "        else:\n",
        "            current_chunk.append(sentence)\n",
        "\n",
        "    # Add the final chunk if any sentences remain\n",
        "    if current_chunk:\n",
        "        chunks.append(' '.join(current_chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Choose either paragraph-based or sentence-based chunking:\n",
        "paragraph_chunks = chunk_document_by_paragraph(cleaned_text)\n",
        "sentence_chunks = chunk_document_by_sentence(cleaned_text)\n",
        "\n",
        "# Print the result\n",
        "print(f\"Paragraph-based chunks: {len(paragraph_chunks)} chunks\")\n",
        "print(f\"Sample paragraph chunk: {paragraph_chunks[0]}\")\n",
        "\n",
        "print(f\"\\nSentence-based chunks: {len(sentence_chunks)} chunks\")\n",
        "print(f\"Sample sentence chunk: {sentence_chunks[0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_hC2m-ZNyKf",
        "outputId": "aa35ec61-2076-4bd8-8d09-1d76316eda1b"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph-based chunks: 225 chunks\n",
            "Sample paragraph chunk: _Being a reprint from the Reminiscences of_ JOHN H. WATSON, M.D., _Late of the Army Medical Department._)     CHAPTER I. MR. SHERLOCK HOLMES.   In the year 1878 I took my degree of Doctor of Medicine of the University of London, and proceeded to Netley to go through the course prescribed for surgeons in the army. Having completed my studies there, I was duly attached to the Fifth Northumberland Fusiliers as Assistant Surgeon. The regiment was stationed in India at the time, and before I could join it, the second Afghan war had broken out. On landing at Bombay, I learned that my corps had advanced through the passes, and was already deep in the enemy’s country. I followed, however, with many other officers who were in the same situation as myself, and succeeded in reaching Candahar in safety, where I found my regiment, and at once entered upon my new duties.  The campaign brought honours and promotion to many, but for me it had nothing but misfortune and disaster. I was removed from my brigade and attached to the Berkshires, with whom I served at the fatal battle of Maiwand. There I was struck on the shoulder by a Jezail bullet, which\n",
            "\n",
            "Sentence-based chunks: 233 chunks\n",
            "Sample sentence chunk: _Being a reprint from the Reminiscences of_ JOHN H. WATSON, M.D.,\n",
            "_Late of the Army Medical Department._)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "CHAPTER I. MR. SHERLOCK HOLMES. In the year 1878 I took my degree of Doctor of Medicine of the\n",
            "University of London, and proceeded to Netley to go through the course\n",
            "prescribed for surgeons in the army. Having completed my studies there,\n",
            "I was duly attached to the Fifth Northumberland Fusiliers as Assistant\n",
            "Surgeon. The regiment was stationed in India at the time, and before I\n",
            "could join it, the second Afghan war had broken out. On landing at\n",
            "Bombay, I learned that my corps had advanced through the passes, and\n",
            "was already deep in the enemy’s country. I followed, however, with many\n",
            "other officers who were in the same situation as myself, and succeeded\n",
            "in reaching Candahar in safety, where I found my regiment, and at once\n",
            "entered upon my new duties. The campaign brought honours and promotion to many, but for me it had\n",
            "nothing but misfortune and disaster. I was removed from my brigade and\n",
            "attached to the Berkshires, with whom I served at the fatal battle of\n",
            "Maiwand.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate sentence embeddings with SBERT.\n",
        "We use this model because it was designed to create rich embeddings preserving semantic similarity\n",
        "\n"
      ],
      "metadata": {
        "id": "ztu8bSIas-94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "def generate_sentence_embeddings(sentences, model=None):\n",
        "    embeddings = model.encode(sentences, show_progress_bar=True)\n",
        "    return embeddings  # Shape: [num_sentences, embedding_dim]\n"
      ],
      "metadata": {
        "id": "ZyZDXzwJtBxe"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load sentence embeddings or generate your own.\n",
        "\n",
        "SBERT will compute the embeddings very fast. Currently taking only a minute or so without a GPU."
      ],
      "metadata": {
        "id": "oRWGxG7mwzSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer('all-MiniLM-L6-v2')  # Or another SBERT model\n",
        "\n",
        "# my_embeddings = torch.load('/content/drive/My Drive/Cosc524 - Collaboration/embeddings/bert_embeddings.pt')\n",
        "my_embeddings = generate_sentence_embeddings(sentence_chunks, model=model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "cf0755db99874f288d715fd44c7fc1ed",
            "09fb4272bcbf42078e484f8ab4334215",
            "270dcc2a8a654076ab627a460cc9394a",
            "f0d8ea372f7b40a8916dd136801b703d",
            "7f0e562924a44f43ae253e7e24caa5c9",
            "917b43c350e34ba29f691e369dd0fe16",
            "75c2381ca4c34ce5b3b8b57db380edf5",
            "1bb17a51a95e4719bc27e2147e1bb82d",
            "ab94a5004b50496ca45242f69b937389",
            "87d1f5ff585347df88d70759228f4ee8",
            "48c384ab2d1546b6b98f7d3e775b6986"
          ]
        },
        "id": "6vo1El1bruMu",
        "outputId": "e6332fe0-77bb-4880-adbe-8b6784bde592"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cf0755db99874f288d715fd44c7fc1ed"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the driver code of RAG context generation.\n",
        "Once we compute our embeddings we use the cosine similarity to understand which sentences are close in the embedded space and retreive the top 'k' closest sentences.  We also need to embed our query for comparison."
      ],
      "metadata": {
        "id": "3U8FQImj2JsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_relevant_info(query, top_k=5, embeddings=None, sentences=None, model=None):\n",
        "\n",
        "    embeddings = torch.tensor(embeddings)\n",
        "    # Compute the query embedding\n",
        "    query_embedding = torch.tensor(model.encode(query)).unsqueeze(0)  # Shape: [1, embedding_dim]\n",
        "\n",
        "    # Compute cosine similarities between query and sentence embeddings\n",
        "    similarities = F.cosine_similarity(embeddings, query_embedding, dim=1)\n",
        "\n",
        "    # Get top-k most similar sentences\n",
        "    top_indices = torch.topk(similarities, top_k).indices\n",
        "\n",
        "    # Retrieve the relevant sentences\n",
        "    relevant_sentences = [sentences[idx] for idx in top_indices]\n",
        "    return relevant_sentences\n"
      ],
      "metadata": {
        "id": "jNUMFUUL6VwW"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate RAG context"
      ],
      "metadata": {
        "id": "tN9UgFC838In"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For text generation\n",
        "generator = pipeline('fill-mask', model='bert-base-uncased')\n",
        "\n",
        "#The model specified here needs to be the same one used to create embeddings\n",
        "#This is because we need to embed the query as well.\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "# model = AutoModel.from_pretrained(\"intfloat/e5-large\")\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"intfloat/e5-large\")\n",
        "\n",
        "#Our generic query will be \"Who is the protagonist in [x]\" where x is our story.\n",
        "query = \"Who is the protagonist in a Study in Scarlet?\"\n",
        "relevant_sentences = retrieve_relevant_info(query, top_k=10, embeddings=my_embeddings, sentences=my_sentences, model=model)\n",
        "context = ' '.join(relevant_sentences)\n",
        "\n",
        "for sentence in relevant_sentences:\n",
        "  print(sentence)\n",
        "\n",
        "#The context looks ok here.\n",
        "print(len(context))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E96nV5TiuJOG",
        "outputId": "f61e451b-462c-43a2-9593-0530841adb8e"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now we have the\n",
            "Sherlock Holmes’ test, and there will no longer be any difficulty.”\n",
            "\n",
            "His eyes fairly glittered as he spoke, and he put his hand over his\n",
            "heart and bowed as if to some applauding crowd conjured up by his\n",
            "imagination.\n",
            "“It\n",
            "seems to me, Stamford,” I added, looking hard at my companion, “that\n",
            "you have some reason for washing your hands of the matter.\n",
            "“My friend here wants to take diggings, and as you were\n",
            "complaining that you could get no one to go halves with you, I thought\n",
            "that I had better bring you together.”\n",
            "\n",
            "Sherlock Holmes seemed delighted at the idea of sharing his rooms with\n",
            "me.\n",
            "I have no\n",
            "doubt, however, that we shall be able to obtain the characteristic\n",
            "reaction.” As he spoke, he threw into the vessel a few white crystals,\n",
            "and then added some drops of a transparent fluid.\n",
            "When it comes to beating the\n",
            "subjects in the dissecting-rooms with a stick, it is certainly taking\n",
            "rather a bizarre shape.”\n",
            "\n",
            "“Beating the subjects!”\n",
            "\n",
            "“Yes, to verify how far bruises may be produced after death.\n",
            "Had this test\n",
            "been invented, there are hundreds of men now walking the earth who\n",
            "would long ago have paid the penalty of their crimes.”\n",
            "\n",
            "“Indeed!” I murmured.\n",
            "Call it the ‘Police\n",
            "News of the Past.’”\n",
            "\n",
            "“Very interesting reading it might be made, too,” remarked Sherlock\n",
            "Holmes, sticking a small piece of plaster over the prick on his finger.\n",
            "I had enough of both in\n",
            "Afghanistan to last me for the remainder of my natural existence.\n",
            "As far as I\n",
            "know he is a decent fellow enough.”\n",
            "\n",
            "“A medical student, I suppose?” said I.\n",
            "“That’s just his little\n",
            "peculiarity,” he said.\n",
            "1580\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing someone elses fine tuned Bert for question answering."
      ],
      "metadata": {
        "id": "2yDwnD9-3E3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "\n",
        "qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "query = \"Who is the protagonist in a Study in Scarlet?\"\n",
        "\n",
        "#If we change the tokenizer in the parameters it generates an incorrect text answer... but atleast it isnt just a comma. ;-;\n",
        "# context = truncate_context(query, context, tokenizer, max_length=512)\n",
        "\n",
        "# Get the answer\n",
        "answer = qa_pipeline(question=query, context=context)[\"answer\"]\n",
        "print('\\n\\n')\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8TcA1rww9tX",
        "outputId": "e5956146-5a80-468a-cbf6-3aa9c459a622"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "Sherlock Holmes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing our fine tuned model"
      ],
      "metadata": {
        "id": "n55luFNa3BNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuned_model_path = \"/content/drive/My Drive/Cosc524 - Collaboration/qa_model/finetuned\"\n",
        "dataset_path = \"/content/drive/My Drive/Cosc524 - Collaboration/dataset/qa_dataset.json\"\n",
        "\n",
        "fine_tuned_model = AutoModelForQuestionAnswering.from_pretrained(fine_tuned_model_path)\n",
        "fine_tuned_tokenizer = AutoTokenizer.from_pretrained(fine_tuned_model_path)\n",
        "\n",
        "# Initialize tokenizer and QA pipeline\n",
        "qa_pipeline = pipeline(\"question-answering\", model=fine_tuned_model, tokenizer=fine_tuned_tokenizer)\n",
        "\n",
        "query = \"Who is the protagonist in a Study in Scarlet?\"\n",
        "\n",
        "# Get the answer\n",
        "answer = qa_pipeline(question=query, context=context)[\"answer\"]\n",
        "print('\\n\\n')\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2w123Ek3NA_",
        "outputId": "37fb7f3e-4b2a-480b-aceb-c35aad3e1cff"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "over\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing out of the box Bert"
      ],
      "metadata": {
        "id": "1FYTNg113LI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize tokenizer and QA pipeline\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
        "\n",
        "qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "query = \"Who is the protagonist in a Study in Scarlet?\"\n",
        "\n",
        "# Get the answer\n",
        "answer = qa_pipeline(question=query, context=context)[\"answer\"]\n",
        "print('\\n\\n')\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4E1wvAD3Ncv",
        "outputId": "b7d2a76b-7148-4cc3-b7e1-ae00eda96fc5"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "you have some reason\n"
          ]
        }
      ]
    }
  ]
}