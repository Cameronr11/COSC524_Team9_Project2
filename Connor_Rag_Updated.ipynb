{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d8c1968735ab4094ad7bab99d98f5262": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c77b0b7fef824229b363029cefcf7416",
              "IPY_MODEL_a9daf838b6e74dcda13bed4d5fa1d5ac",
              "IPY_MODEL_59862729aca04018aa16f3bfa7bfe47a"
            ],
            "layout": "IPY_MODEL_8404e8c50638421c8fde26aa569c153c"
          }
        },
        "c77b0b7fef824229b363029cefcf7416": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06202e1766fd439fbd8ade53a8f21a0c",
            "placeholder": "​",
            "style": "IPY_MODEL_880adc57e2364dad8e44ef6e60f8ab42",
            "value": "model.safetensors: 100%"
          }
        },
        "a9daf838b6e74dcda13bed4d5fa1d5ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1746f9c52e3c4dcc8747bc2b5bacceed",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7d7d1c8c55bf477582b1c4d5e3740076",
            "value": 440449768
          }
        },
        "59862729aca04018aa16f3bfa7bfe47a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bbe2292c94ce48c9b485957477ecda47",
            "placeholder": "​",
            "style": "IPY_MODEL_2fd5c6ff6d7543fdb03d6680f17c0792",
            "value": " 440M/440M [00:02&lt;00:00, 195MB/s]"
          }
        },
        "8404e8c50638421c8fde26aa569c153c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06202e1766fd439fbd8ade53a8f21a0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "880adc57e2364dad8e44ef6e60f8ab42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1746f9c52e3c4dcc8747bc2b5bacceed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d7d1c8c55bf477582b1c4d5e3740076": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bbe2292c94ce48c9b485957477ecda47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fd5c6ff6d7543fdb03d6680f17c0792": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJWe40Kqpe9-",
        "outputId": "c30ef221-7dbf-44cc-b3e9-c75fbdc68261"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch datasets\n",
        "from transformers import BertTokenizer, BertModel, pipeline, AutoTokenizer, AutoModelForQuestionAnswering, Trainer, TrainingArguments, AutoModelForQuestionAnswering\n",
        "from datasets import load_dataset, Dataset\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import re\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_text(file_path):\n",
        "    \"\"\"\n",
        "    Load text from a file.\n",
        "    \"\"\"\n",
        "    print(\"Loading the text from the file...\")\n",
        "    with open(book_path, 'r', encoding='utf-8') as file:\n",
        "        return file.read()\n",
        "\n",
        "def remove_gutenberg_header_footer(text, start_marker, end_marker):\n",
        "    \"\"\"\n",
        "    Remove the header and footer from Project Gutenberg text.\n",
        "\n",
        "    Finds specified start and end markers and returns the content in between.\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The text to clean.\n",
        "        start_marker (str): Marker indicating the start of main content.\n",
        "        end_marker (str): Marker indicating the end of main content.\n",
        "\n",
        "    Returns:\n",
        "        str: Text with header and footer removed, or original text if markers are not found.\n",
        "    \"\"\"\n",
        "    start_index = text.find(start_marker)\n",
        "    if start_index == -1:\n",
        "        return text\n",
        "\n",
        "    end_index = text.find(end_marker)\n",
        "    if end_index == -1:\n",
        "        return text\n",
        "\n",
        "    cleaned_text = text[start_index + len(start_marker):end_index].strip()\n",
        "    return cleaned_text\n",
        "\n",
        "book_path = \"/content/drive/My Drive/Cosc524 - Collaboration/a study in scarlet.txt\"\n",
        "\n",
        "# here we are cleaning the text and removing the headers and footers\n",
        "raw_text = load_text(book_path)\n",
        "\n",
        "start_marker = \"*** START OF THE PROJECT GUTENBERG EBOOK A STUDY IN SCARLET ***\"\n",
        "end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK A STUDY IN SCARLET ***\"\n",
        "\n",
        "cleaned_text = remove_gutenberg_header_footer(raw_text, start_marker, end_marker)\n",
        "\n",
        "print(\"Cleaned text sample:\\n\")\n",
        "print(cleaned_text[:500])\n",
        "\n",
        "def tokenize_sentences(text):\n",
        "    \"\"\"\n",
        "    Tokenize the input text into sentences using NLTK.\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The text to tokenize.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of tokenized sentences.\n",
        "    \"\"\"\n",
        "    print(\"Tokenizing the text into sentences...\")\n",
        "    sentences = sent_tokenize(text)\n",
        "    print(f\"Total sentences: {len(sentences)}\")\n",
        "    return sentences\n",
        "\n",
        "my_sentences = tokenize_sentences(cleaned_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aplu0ANdyoww",
        "outputId": "362c4d55-d0e9-40a6-a836-e2031d12254a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the text from the file...\n",
            "Cleaned text sample:\n",
            "\n",
            "A STUDY IN SCARLET\n",
            "\n",
            "By A. Conan Doyle\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "CONTENTS\n",
            "\n",
            " A STUDY IN SCARLET.\n",
            "\n",
            " PART I.\n",
            " CHAPTER I. MR. SHERLOCK HOLMES.\n",
            " CHAPTER II. THE SCIENCE OF DEDUCTION.\n",
            " CHAPTER III. THE LAURISTON GARDENS MYSTERY\n",
            " CHAPTER IV. WHAT JOHN RANCE HAD TO TELL.\n",
            " CHAPTER V. OUR ADVERTISEMENT BRINGS A VISITOR.\n",
            " CHAPTER VI. TOBIAS GREGSON SHOWS WHAT HE CAN DO.\n",
            " CHAPTER VII. LIGHT IN THE DARKNESS.\n",
            "\n",
            " PART II. THE COUNTRY OF THE SAINTS\n",
            " CHAPTER I. ON THE GREAT ALKALI PLAIN.\n",
            " CHAPTER II. THE FLOWER OF UTAH.\n",
            " CHAPTER III. J\n",
            "Tokenizing the text into sentences...\n",
            "Total sentences: 2208\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # For text generation\n",
        "# generator = pipeline('fill-mask', model='bert-base-uncased')\n",
        "\n",
        "# # For text retrieval\n",
        "\n",
        "# #I should try swapping to the question answer BERT model!\n",
        "# retriever_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# retriever_model = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208,
          "referenced_widgets": [
            "d8c1968735ab4094ad7bab99d98f5262",
            "c77b0b7fef824229b363029cefcf7416",
            "a9daf838b6e74dcda13bed4d5fa1d5ac",
            "59862729aca04018aa16f3bfa7bfe47a",
            "8404e8c50638421c8fde26aa569c153c",
            "06202e1766fd439fbd8ade53a8f21a0c",
            "880adc57e2364dad8e44ef6e60f8ab42",
            "1746f9c52e3c4dcc8747bc2b5bacceed",
            "7d7d1c8c55bf477582b1c4d5e3740076",
            "bbe2292c94ce48c9b485957477ecda47",
            "2fd5c6ff6d7543fdb03d6680f17c0792"
          ]
        },
        "id": "JhD7gkdhpi_m",
        "outputId": "3bd0d0a0-1748-4445-854b-74266cd8fc94"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d8c1968735ab4094ad7bab99d98f5262"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
            "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
            "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
            "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def retrieve_relevant_info(query, top_k=5, embeddings=None, sentences=None):\n",
        "\n",
        "#   \"\"\"Retrieves the most relevant sentences based on cosine similarity.\n",
        "\n",
        "#   Args:\n",
        "#     query: The input query string.\n",
        "#     top_k: The number of top sentences to retrieve.\n",
        "\n",
        "#   Returns:\n",
        "#     A list of the top_k most relevant sentences.\n",
        "#   \"\"\"\n",
        "#   query_embedding = retriever_model(**retriever_tokenizer(query, return_tensors=\"pt\")).last_hidden_state[:, 0, :]\n",
        "#   similarities = F.cosine_similarity(embeddings, query_embedding, dim=1) # Assume 'embeddings' is pre-calculated\n",
        "#   top_indices = torch.topk(similarities, top_k).indices\n",
        "#   relevant_sentences = [sentences[idx] for idx in top_indices]  # Assume 'sentences' is pre-calculated\n",
        "#   return relevant_sentences"
      ],
      "metadata": {
        "id": "Wjmdx5N4rmI5"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def generate_text_with_rag(prompt, top_k=5, embeddings=None, sentences=None):\n",
        "#   \"\"\"Generates text using RAG with BERT.\n",
        "\n",
        "#   Args:\n",
        "#     prompt: The input prompt for text generation.\n",
        "#     top_k: The number of retrieved sentences to use for augmentation.\n",
        "\n",
        "#   Returns:\n",
        "#     The generated text.\n",
        "#   \"\"\"\n",
        "#   relevant_info = retrieve_relevant_info(prompt, top_k=top_k, embeddings=embeddings, sentences=sentences)\n",
        "#   augmented_prompt = f\"{prompt} [SEP] {' '.join(relevant_info)} [MASK].\"  # Add [MASK] token\n",
        "#   generated_text = generator(augmented_prompt)[0]['sequence']\n",
        "#   return generated_text"
      ],
      "metadata": {
        "id": "ly_4yQ3crsQM"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load word embeddings"
      ],
      "metadata": {
        "id": "oRWGxG7mwzSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_embeddings = torch.load('/content/drive/My Drive/Cosc524 - Collaboration/embeddings/bert_embeddings.pt')\n",
        "# prompt = \"Who was the murderer?\"\n",
        "# generated_text = generate_text_with_rag(prompt=prompt, top_k=5, embeddings=my_embeddings, sentences=my_sentences)\n",
        "# # print(generated_text)\n",
        "# for sentence in sent_tokenize(generated_text):\n",
        "#   print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vo1El1bruMu",
        "outputId": "54b1c887-c9a7-4552-d52c-6d728c93ba9d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-e99a740c6d6a>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  my_embeddings = torch.load('/content/drive/My Drive/Cosc524 - Collaboration/embeddings/bert_embeddings.pt')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Working on RAG doot doot, doot doot"
      ],
      "metadata": {
        "id": "sZGIY0ARgA53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline('question-answering', model='distilbert-base-uncased-distilled-squad')\n",
        "\n",
        "def generate_text_with_rag(prompt, top_k=5, embeddings=None, sentences=None):\n",
        "    relevant_info = retrieve_relevant_info(prompt, top_k=top_k, embeddings=embeddings, sentences=sentences)\n",
        "    context = \" \".join(relevant_info)  # Combine retrieved sentences as context\n",
        "    generated_text = generator(question=prompt, context=context)['answer']\n",
        "    return generated_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHd0hruBgFOU",
        "outputId": "e41222d0-a6fd-480a-b3a6-3f09748559e7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve relevant sentences based on query\n",
        "def retrieve_relevant_info(query, top_k=5, embeddings=None, sentences=None):\n",
        "    \"\"\"\n",
        "    Retrieves the most relevant sentences based on cosine similarity with the query.\n",
        "\n",
        "    Args:\n",
        "        query: Input query string.\n",
        "        top_k: Number of top sentences to retrieve.\n",
        "        embeddings: Precomputed sentence embeddings.\n",
        "        sentences: List of sentences corresponding to the embeddings.\n",
        "\n",
        "    Returns:\n",
        "        A list of the top_k most relevant sentences.\n",
        "    \"\"\"\n",
        "    # Compute query embedding\n",
        "    query_input = retriever_tokenizer(query, return_tensors=\"pt\", truncation=True)\n",
        "    with torch.no_grad():\n",
        "        query_output = retriever_model(**query_input)\n",
        "        query_embedding = query_output.last_hidden_state.mean(dim=1)  # Average over tokens\n",
        "\n",
        "    # Compute cosine similarities\n",
        "    similarities = F.cosine_similarity(embeddings, query_embedding, dim=1)\n",
        "\n",
        "    # Get top-k most similar sentences\n",
        "    top_indices = torch.topk(similarities, top_k).indices\n",
        "    relevant_sentences = [sentences[idx] for idx in top_indices]\n",
        "    return relevant_sentences\n"
      ],
      "metadata": {
        "id": "o18GQfyugN7z"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate RAG context"
      ],
      "metadata": {
        "id": "-OtwG0PfIFJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For text generation\n",
        "generator = pipeline('fill-mask', model='bert-base-uncased')\n",
        "\n",
        "#I should try swapping to the question answer BERT model!\n",
        "retriever_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "retriever_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Example usage\n",
        "query = \"Who is the murderer in A Study in Scarlet?\"\n",
        "relevant_sentences = retrieve_relevant_info(query, top_k=5, embeddings=my_embeddings, sentences=my_sentences)\n",
        "context = ' '.join(relevant_sentences)\n",
        "# print(\"Relevant Sentences:\", relevant_sentences)\n",
        "for sentence in relevant_sentences:\n",
        "  print(sentence)\n",
        "\n",
        "#The context looks ok here.\n",
        "print(len(context))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfunLWLYIDnI",
        "outputId": "58c1e5b1-da69-40d6-9e58-237b17d8ca60"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Are they blood stains, or mud stains, or rust stains, or fruit\n",
            "stains, or what are they?\n",
            "A man is\n",
            "suspected of a crime months perhaps after it has been committed.\n",
            "What has become of\n",
            "the cabman who drove them?\n",
            "The\n",
            "tradesman was put to his trade and the artisan to his calling.\n",
            "JOHN FERRIER TALKS WITH THE PROPHET.\n",
            "312\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#truncate context to match BERT context window size\n",
        "#The model I am using is 512 window size\n",
        "def truncate_context(question, context, tokenizer, max_length=512):\n",
        "    \"\"\"\n",
        "    Truncates the context to fit within the model's max token limit.\n",
        "\n",
        "    Args:\n",
        "        question: The input question.\n",
        "        context: The input context string.\n",
        "        tokenizer: The tokenizer used to calculate token length.\n",
        "        max_length: The maximum allowed length of tokens.\n",
        "\n",
        "    Returns:\n",
        "        A truncated context string.\n",
        "    \"\"\"\n",
        "    tokens = tokenizer(question, context, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
        "    return tokenizer.decode(tokens[\"input_ids\"][0], skip_special_tokens=True)\n",
        "\n",
        "print(len(context))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RWgv8vQgaa0",
        "outputId": "7068be46-2292-486e-d0d2-a9c0d22da052"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "312\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuned_model_path = \"/content/drive/My Drive/Cosc524 - Collaboration/qa_model/finetuned\"\n",
        "dataset_path = \"/content/drive/My Drive/Cosc524 - Collaboration/dataset/qa_dataset.json\"\n",
        "\n",
        "print(\"Loading the fine-tuned model...\")\n",
        "fine_tuned_model = AutoModelForQuestionAnswering.from_pretrained(fine_tuned_model_path)\n",
        "fine_tuned_tokenizer = AutoTokenizer.from_pretrained(fine_tuned_model_path)\n",
        "\n",
        "# Initialize tokenizer and QA pipeline\n",
        "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "qa_pipeline = pipeline(\"question-answering\", model=fine_tuned_model, tokenizer=fine_tuned_tokenizer)\n",
        "\n",
        "query = \"Who is the murderer in A Study in Scarlet? Was it a cat?\"\n",
        "\n",
        "#If we change the tokenizer in the parameters it generates an incorrect text answer... but atleast it isnt just a comma. ;-;\n",
        "context = truncate_context(query, context, fine_tuned_tokenizer, max_length=512)\n",
        "\n",
        "# Get the answer\n",
        "answer = qa_pipeline(question=query, context=context)[\"answer\"]\n",
        "print('\\n\\n')\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "QcFGlUmLH8WY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0630d1c3-726e-4d0a-fba3-99112af69364"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the fine-tuned model...\n",
            "\n",
            "\n",
            "\n",
            "tradesman was put to\n"
          ]
        }
      ]
    }
  ]
}